the number of examples a [[Machine Learning]] model processes before updating its weights and bias

In a [[Gradient Descent]] example, the whole batch might be too big to test every single loss, so for example, we hav 1 million point to test the loss, it's simply to big to test 1 million examples every single time, and we could have a batch size of 100 examples instead