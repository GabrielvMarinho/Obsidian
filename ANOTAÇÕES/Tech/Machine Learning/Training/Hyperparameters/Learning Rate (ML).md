sets how quickly a [[Machine Learning]] model converges

basically how big of a jump the model is going to do in It's training, for example, test a bias and weight of 1 percent increase each time, or could be higher, jumping 10 percent after each test, of course it could increment and decrement to get close to the lowest loss, in this [[Gradient Descent]] example.
If It's to low, It's going to take a lot of test to get close to the lowest loss, while with a higher rate, It can gain acurrateness faster but the high value in learning rate just makes the model bounces up and down, while the lowest loss lies in the middle
